{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":121144,"databundleVersionId":14484960,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **LLM SELF-EVALUATION & PROMPT-FINETUNING SYSTEM-**\n\n**1 .Agent2Agent + Memory Retrieval/Update + Context Engineering**\n\n**2 .Using Google Gemini API**\n","metadata":{}},{"cell_type":"markdown","source":"# üéØ **Notebook Usage Guide for this project**\n\n---\n\n## üîê **1. One-Time Setup ‚Äî Add Gemini API Key (Kaggle Secrets)**\n\n**Follow these steps before running any code:**\n\n1. Get your **free API key**:  \n   https://makersuite.google.com/app/apikey  \n2. In the Kaggle notebook menu:  \n   **Add-ons ‚Üí Secrets**\n3. Click **‚ÄúAdd a new secret‚Äù**\n4. Fill the fields:\n   - **Label:** `GEMINI_API_KEY`  \n   - **Secret:** *your actual Gemini API key*\n5. Toggle it **ON** ‚úîÔ∏è  \n6. Click **Save**\n\n---\n\n## üöÄ **2. Running the Notebook (Cell-by-Cell Guide)**\n\n### ‚ñ∂Ô∏è **Cell Execution Order**\n\n**Cell 1** ‚Äî Install libraries  \n_Only once (takes ~2 minutes)._\n\n**Cell 2** ‚Äî Import libraries\n\n**Cell 3** ‚Äî Automatically loads your API key from Kaggle Secrets  \n(Optional: tweak config parameters here)\n\n**Cells 4‚Äì11** ‚Äî Run all cells in order\n\n**Cell 11** ‚Äî ‚úèÔ∏è Change this line:\ntask = \"...\"\n\nPut your custom instruction here.\n\n**Cell 12** ‚Äî Run the agent workflow\n\n---\n\n## üìù **3. Editable Parts (What You Can Change)**\n**üîß Cell 11**\n\nReplace:\n\n```python\ntask = \"...\"\n```\nwith your desired task.\n\n**‚öôÔ∏è Cell 3 (Lines ~42‚Äì50) ‚Äî Configuration Options**\n\nYou may modify:\n\nNumber of iterations\n\nScoring threshold\n\nSelected model\n\nVerbosity\n\nTemperature / randomness settings\n\n---\n\n## ü§ñ **4. Model Options**\n\nYou can select any of these models:\n\ngemini-1.5-flash ‚Äî ‚ö° Fastest & Free (Default)\n\ngemini-1.5-pro ‚Äî More capable\n\ngemini-2.0-flash-exp ‚Äî Experimental version\n\n---\n\n## üìä **5. What Outputs You Will See**\n\nüñ•Ô∏è Console logs for every iteration\n\nü§ñ Final improved response\n\nüìà Score progression graph (if enabled)\n\nüì¶ Full output dictionary available as:\n\n```bash\nresults\n```\n\n---\n\n## üÜì **6. Why Use Gemini API?**\n\nGenerous free tier\n\nVery fast inference\n\nGreat multilingual support\n\nNo credit card needed","metadata":{"execution":{"iopub.status.busy":"2025-11-26T05:06:34.060777Z","iopub.execute_input":"2025-11-26T05:06:34.061095Z","iopub.status.idle":"2025-11-26T05:06:34.071150Z","shell.execute_reply.started":"2025-11-26T05:06:34.061070Z","shell.execute_reply":"2025-11-26T05:06:34.069846Z"}}},{"cell_type":"code","source":"import os\nimport warnings\n\n# Suppress Python warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Disable HuggingFace tokenizers parallelism warning\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n!pip install -qq --disable-pip-version-check --no-warn-script-location \\\n    google-generativeai chromadb sentence-transformers matplotlib ipywidgets --upgrade > /dev/null 2>&1\n\nprint(\"‚úÖ Libraries installed successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T06:22:54.169393Z","iopub.execute_input":"2025-11-27T06:22:54.169827Z","iopub.status.idle":"2025-11-27T06:23:00.327833Z","shell.execute_reply.started":"2025-11-27T06:22:54.169770Z","shell.execute_reply":"2025-11-27T06:23:00.326412Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Libraries installed successfully!\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# Enable inline plotting (required in Kaggle)\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nprint(\"‚úÖ Enabling inline plotting.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T06:23:04.661389Z","iopub.execute_input":"2025-11-27T06:23:04.661791Z","iopub.status.idle":"2025-11-27T06:23:04.671018Z","shell.execute_reply.started":"2025-11-27T06:23:04.661741Z","shell.execute_reply":"2025-11-27T06:23:04.669973Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Enabling inline plotting.\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# Import Libraries\n\nimport json\nimport os\nfrom typing import List, Dict, Any, Optional\nfrom dataclasses import dataclass, asdict\nfrom datetime import datetime\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\nimport chromadb\nfrom chromadb.config import Settings\nimport google.generativeai as genai\n\nprint(\"‚úÖ All Libraries imported.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T06:23:07.025022Z","iopub.execute_input":"2025-11-27T06:23:07.025398Z","iopub.status.idle":"2025-11-27T06:23:07.032676Z","shell.execute_reply.started":"2025-11-27T06:23:07.025371Z","shell.execute_reply":"2025-11-27T06:23:07.031581Z"}},"outputs":[{"name":"stdout","text":"‚úÖ All Libraries imported.\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# Test Cell: Verify Installation\nprint(\"Testing installations...\")\n\ntry:\n    import google.generativeai as genai\n    print(\"‚úÖ Google Generative AI: OK\")\nexcept:\n    print(\"‚ùå Google Generative AI: FAILED\")\n\ntry:\n    import chromadb\n    print(\"‚úÖ ChromaDB: OK\")\nexcept:\n    print(\"‚ùå ChromaDB: FAILED\")\n\ntry:\n    from sentence_transformers import SentenceTransformer\n    print(\"‚úÖ Sentence Transformers: OK\")\nexcept:\n    print(\"‚ùå Sentence Transformers: FAILED\")\n\ntry:\n    import matplotlib.pyplot as plt\n    print(\"‚úÖ Matplotlib: OK\")\nexcept:\n    print(\"‚ùå Matplotlib: FAILED\")\n\nprint(\"\\n‚ú® All critical libraries working!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T06:23:12.232576Z","iopub.execute_input":"2025-11-27T06:23:12.232937Z","iopub.status.idle":"2025-11-27T06:23:12.240818Z","shell.execute_reply.started":"2025-11-27T06:23:12.232911Z","shell.execute_reply":"2025-11-27T06:23:12.239286Z"}},"outputs":[{"name":"stdout","text":"Testing installations...\n‚úÖ Google Generative AI: OK\n‚úÖ ChromaDB: OK\n‚úÖ Sentence Transformers: OK\n‚úÖ Matplotlib: OK\n\n‚ú® All critical libraries working!\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# Configuration (Using Kaggle Secrets!)\nclass Config:\n    \"\"\"Configuration for the LLM Agent System\"\"\"\n    \n    # üîê AUTOMATICALLY LOADS FROM KAGGLE SECRETS\n    try:\n        from kaggle_secrets import UserSecretsClient\n        user_secrets = UserSecretsClient()\n        GEMINI_API_KEY = user_secrets.get_secret(\"GEMINI_API_KEY\")\n        print(\"‚úÖ Gemini API Key loaded successfully!\")\n        \n        # Configure Gemini\n        import google.generativeai as genai\n        genai.configure(api_key=GEMINI_API_KEY)\n        \n        # List available models (for debugging)\n        print(\"\\nüìã Available models:\")\n        for m in genai.list_models():\n            if 'generateContent' in m.supported_generation_methods:\n                print(f\"  ‚úì {m.name}\")\n        \n    except Exception as e:\n        print(f\"‚ö†Ô∏è Could not load API key: {e}\")\n        print(\"\\nüìù Setup Instructions:\")\n        print(\"   1. Get API key: https://makersuite.google.com/app/apikey\")\n        print(\"   2. Kaggle ‚Üí Add-ons ‚Üí Secrets\")\n        print(\"   3. Add: GEMINI_API_KEY = your-key\")\n        print(\"   4. Turn ON toggle\")\n        GEMINI_API_KEY = None\n    \n    # ‚ö†Ô∏è UPDATED MODEL NAMES\n    MODEL_NAME = \"models/gemini-2.5-flash\"\n    MAX_TOKENS = 2000\n    TEMPERATURE = 0.7\n    \n    EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n    MEMORY_COLLECTION_NAME = \"agent_memory\"\n    TOP_K_MEMORIES = 5\n    MAX_ITERATIONS = 5\n    EVALUATION_THRESHOLD = 0.7","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T06:23:15.370725Z","iopub.execute_input":"2025-11-27T06:23:15.371758Z","iopub.status.idle":"2025-11-27T06:23:15.816061Z","shell.execute_reply.started":"2025-11-27T06:23:15.371724Z","shell.execute_reply":"2025-11-27T06:23:15.814827Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Gemini API Key loaded successfully!\n\nüìã Available models:\n  ‚úì models/gemini-2.5-pro-preview-03-25\n  ‚úì models/gemini-2.5-flash\n  ‚úì models/gemini-2.5-pro-preview-05-06\n  ‚úì models/gemini-2.5-pro-preview-06-05\n  ‚úì models/gemini-2.5-pro\n  ‚úì models/gemini-2.0-flash-exp\n  ‚úì models/gemini-2.0-flash\n  ‚úì models/gemini-2.0-flash-001\n  ‚úì models/gemini-2.0-flash-exp-image-generation\n  ‚úì models/gemini-2.0-flash-lite-001\n  ‚úì models/gemini-2.0-flash-lite\n  ‚úì models/gemini-2.0-flash-lite-preview-02-05\n  ‚úì models/gemini-2.0-flash-lite-preview\n  ‚úì models/gemini-2.0-pro-exp\n  ‚úì models/gemini-2.0-pro-exp-02-05\n  ‚úì models/gemini-exp-1206\n  ‚úì models/gemini-2.0-flash-thinking-exp-01-21\n  ‚úì models/gemini-2.0-flash-thinking-exp\n  ‚úì models/gemini-2.0-flash-thinking-exp-1219\n  ‚úì models/gemini-2.5-flash-preview-tts\n  ‚úì models/gemini-2.5-pro-preview-tts\n  ‚úì models/learnlm-2.0-flash-experimental\n  ‚úì models/gemma-3-1b-it\n  ‚úì models/gemma-3-4b-it\n  ‚úì models/gemma-3-12b-it\n  ‚úì models/gemma-3-27b-it\n  ‚úì models/gemma-3n-e4b-it\n  ‚úì models/gemma-3n-e2b-it\n  ‚úì models/gemini-flash-latest\n  ‚úì models/gemini-flash-lite-latest\n  ‚úì models/gemini-pro-latest\n  ‚úì models/gemini-2.5-flash-lite\n  ‚úì models/gemini-2.5-flash-image-preview\n  ‚úì models/gemini-2.5-flash-image\n  ‚úì models/gemini-2.5-flash-preview-09-2025\n  ‚úì models/gemini-2.5-flash-lite-preview-09-2025\n  ‚úì models/gemini-3-pro-preview\n  ‚úì models/gemini-3-pro-image-preview\n  ‚úì models/nano-banana-pro-preview\n  ‚úì models/gemini-robotics-er-1.5-preview\n  ‚úì models/gemini-2.5-computer-use-preview-10-2025\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# Memory System with Vector Database\n\n# This handles storing and retrieving relevant past interactions\nclass MemorySystem:\n    \"\"\"Handles memory storage, retrieval, and updates using vector embeddings\"\"\"\n    \n    def __init__(self):\n        # Initialize embedding model for semantic search\n        self.embedding_model = SentenceTransformer(Config.EMBEDDING_MODEL)\n        \n        # Initialize ChromaDB for vector storage\n        self.client = chromadb.Client(Settings(anonymized_telemetry=False))\n        \n        # Create or get existing collection\n        try:\n            self.collection = self.client.get_collection(Config.MEMORY_COLLECTION_NAME)\n        except:\n            self.collection = self.client.create_collection(\n                name=Config.MEMORY_COLLECTION_NAME,\n                metadata={\"description\": \"Agent interaction memory\"}\n            )\n        \n        self.memory_count = 0\n    \n    def add_memory(self, content: str, metadata: Dict[str, Any]) -> str:\n        \"\"\"Add a new memory with semantic embedding\"\"\"\n        # Create unique ID for this memory\n        memory_id = f\"mem_{self.memory_count}_{datetime.now().timestamp()}\"\n        \n        # Convert text to vector embedding\n        embedding = self.embedding_model.encode(content).tolist()\n        \n        # Store in vector database\n        self.collection.add(\n            embeddings=[embedding],\n            documents=[content],\n            metadatas=[metadata],\n            ids=[memory_id]\n        )\n        \n        self.memory_count += 1\n        return memory_id\n    \n    def retrieve_memories(self, query: str, top_k: int = Config.TOP_K_MEMORIES) -> List[Dict]:\n        \"\"\"Retrieve relevant memories based on semantic similarity\"\"\"\n        # Convert query to embedding\n        query_embedding = self.embedding_model.encode(query).tolist()\n        \n        # Search for similar memories\n        results = self.collection.query(\n            query_embeddings=[query_embedding],\n            n_results=top_k\n        )\n        \n        # Format results\n        memories = []\n        if results['documents'] and results['documents'][0]:\n            for i, doc in enumerate(results['documents'][0]):\n                memories.append({\n                    'content': doc,\n                    'metadata': results['metadatas'][0][i] if results['metadatas'] else {},\n                    'distance': results['distances'][0][i] if results['distances'] else 0\n                })\n        \n        return memories\n    \n    def update_memory(self, memory_id: str, content: str, metadata: Dict[str, Any]):\n        \"\"\"Update an existing memory\"\"\"\n        # Delete old memory\n        try:\n            self.collection.delete(ids=[memory_id])\n        except:\n            pass\n        \n        # Add updated memory\n        embedding = self.embedding_model.encode(content).tolist()\n        self.collection.add(\n            embeddings=[embedding],\n            documents=[content],\n            metadatas=[metadata],\n            ids=[memory_id]\n        )\n    \n    def get_all_memories(self) -> List[Dict]:\n        \"\"\"Retrieve all stored memories\"\"\"\n        try:\n            results = self.collection.get()\n            memories = []\n            if results['documents']:\n                for i, doc in enumerate(results['documents']):\n                    memories.append({\n                        'id': results['ids'][i],\n                        'content': doc,\n                        'metadata': results['metadatas'][i] if results['metadatas'] else {}\n                    })\n            return memories\n        except:\n            return []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T06:23:19.817544Z","iopub.execute_input":"2025-11-27T06:23:19.817889Z","iopub.status.idle":"2025-11-27T06:23:19.831997Z","shell.execute_reply.started":"2025-11-27T06:23:19.817865Z","shell.execute_reply":"2025-11-27T06:23:19.830732Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"class ContextEngineering:\n    \"\"\"Manages context construction and optimization for LLM prompts\"\"\"\n    \n    @staticmethod\n    def build_context(\n        task: str,\n        relevant_memories: List[Dict],\n        conversation_history: List[Dict],\n        system_prompt: str = \"\"\n    ) -> str:\n        \"\"\"\n        Build optimized, smooth, and clean context for LLM prompts.\n        Ensures consistent formatting, stable spacing, and predictable structure.\n        \"\"\"\n        \n        context_parts = []\n\n        # === 1. System Instructions ===\n        if system_prompt and system_prompt.strip():\n            context_parts.append(\n                \"### System Instructions\\n\"\n                f\"{system_prompt.strip()}\\n\"\n            )\n\n        # === 2. Relevant Memories ===\n        if relevant_memories:\n            mem_lines = []\n            for mem in relevant_memories[:3]:   # Use only top 3\n                score = 1 - mem.get(\"distance\", 0)\n                mem_lines.append(f\"- {mem['content']}  (score: {score:.2f})\")\n            \n            context_parts.append(\n                \"### Relevant Memories\\n\"\n                + \"\\n\".join(mem_lines) + \"\\n\"\n            )\n\n        # === 3. Recent Conversation History ===\n        if conversation_history:\n            hist_lines = []\n            for msg in conversation_history[-3:]:\n                role = msg.get(\"role\", \"unknown\").capitalize()\n                content = msg.get(\"content\", \"\").strip()\n                hist_lines.append(f\"{role}: {content}\")\n            \n            context_parts.append(\n                \"### Conversation History\\n\"\n                + \"\\n\".join(hist_lines) + \"\\n\"\n            )\n\n        # === 4. Current Task ===\n        context_parts.append(\n            \"### Current Task\\n\"\n            f\"{task.strip()}\"\n        )\n\n        # === Final Clean Join ===\n        final_context = \"\\n\".join(context_parts).strip()\n\n        return final_context\n    \n    @staticmethod\n    def compress_context(context: str, max_length: int = 3000) -> str:\n        \"\"\"\n        Compress context if too large. \n        Preserves start and end so the LLM retains global structure.\n        \"\"\"\n        if len(context) <= max_length:\n            return context\n        \n        portion = max_length // 3\n        \n        return (\n            context[:portion].rstrip() +\n            \"\\n\\n[... context compressed for length ...]\\n\\n\" +\n            context[-portion:].lstrip()\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T06:23:24.419484Z","iopub.execute_input":"2025-11-27T06:23:24.419984Z","iopub.status.idle":"2025-11-27T06:23:24.431072Z","shell.execute_reply.started":"2025-11-27T06:23:24.419948Z","shell.execute_reply":"2025-11-27T06:23:24.429990Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# Agent Base Class\n\n# Foundation for all specialized agents\n@dataclass\nclass AgentResponse:\n    \"\"\"Standard response format for agents\"\"\"\n    content: str  # The actual response\n    confidence: float  # Confidence score (0-1)\n    reasoning: str  # Why this response was generated\n    metadata: Dict[str, Any]  # Additional info\n\nclass BaseAgent:\n    \"\"\"Base class for all agents - provides common functionality\"\"\"\n    \n    def __init__(self, name: str, role: str, system_prompt: str):\n        self.name = name\n        self.role = role\n        self.system_prompt = system_prompt\n        self.memory = MemorySystem()  # Each agent has its own memory\n        self.conversation_history = []\n    \n    def call_llm(self, prompt: str) -> str:\n        \"\"\"Call the Gemini LLM API - This is where the magic happens!\"\"\"\n        try:\n            # Check if API key is available\n            if not Config.GEMINI_API_KEY:\n                raise ValueError(\"API key not configured. Please add GEMINI_API_KEY to Kaggle Secrets.\")\n            \n            # Initialize Gemini model\n            model = genai.GenerativeModel(\n                model_name=Config.MODEL_NAME,\n                generation_config={\n                    \"temperature\": Config.TEMPERATURE,\n                    \"max_output_tokens\": Config.MAX_TOKENS,\n                }\n            )\n            \n            # Call Gemini API\n            response = model.generate_content(prompt)\n            \n            # Extract text response\n            return response.text\n            \n        except Exception as e:\n            # If API fails, provide helpful error message\n            print(f\"‚ö†Ô∏è API Error: {e}\")\n            print(\"üí° Setup Kaggle Secrets:\")\n            print(\"   1. Get API key from: https://makersuite.google.com/app/apikey\")\n            print(\"   2. Click 'Add-ons' ‚Üí 'Secrets' in Kaggle\")\n            print(\"   3. Add secret: GEMINI_API_KEY = your-key\")\n            print(\"   4. Turn ON the toggle switch\")\n            print(\"   5. Re-run the notebook\")\n            # Return simulation for testing without API\n            return f\"[Simulated response - API not configured]\\nTask: {prompt[:100]}...\"\n    \n    def process(self, task: str, context: Optional[str] = None) -> AgentResponse:\n        \"\"\"Process a task with context - Override in subclasses\"\"\"\n        raise NotImplementedError(\"Subclasses must implement process method\")\n\n# === Confirmation output ===\nprint(\"‚úÖ Agent Base Class set-up done.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T06:23:28.495732Z","iopub.execute_input":"2025-11-27T06:23:28.496091Z","iopub.status.idle":"2025-11-27T06:23:28.507051Z","shell.execute_reply.started":"2025-11-27T06:23:28.496067Z","shell.execute_reply":"2025-11-27T06:23:28.505844Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Agent Base Class set-up done.\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# Worker Agent (Does the actual work)\n\nclass WorkerAgent(BaseAgent):\n    \"\"\"Agent that performs the actual task execution\"\"\"\n    \n    def __init__(self):\n        super().__init__(\n            name=\"Worker\",\n            role=\"Task Executor\",\n            system_prompt=\"\"\"You are a helpful AI assistant that executes tasks.\n            Provide clear, accurate, and well-reasoned responses.\n            Explain your reasoning step by step.\n            Be specific and practical in your answers.\"\"\"\n        )\n    \n    def process(self, task: str, context: Optional[str] = None) -> AgentResponse:\n        \"\"\"Execute the given task with full context\"\"\"\n        \n        # Step 1: Retrieve relevant past memories\n        relevant_memories = self.memory.retrieve_memories(task)\n        \n        # Step 2: Build comprehensive context\n        full_context = ContextEngineering.build_context(\n            task=task,\n            relevant_memories=relevant_memories,\n            conversation_history=self.conversation_history,\n            system_prompt=self.system_prompt\n        )\n        \n        # Step 3: Construct final prompt\n        prompt = f\"{full_context}\\n\\nPlease provide a detailed response with your reasoning.\"\n        \n        # Step 4: Call LLM to get response\n        response = self.call_llm(prompt)\n        \n        # Step 5: Store this interaction in memory for future use\n        self.memory.add_memory(\n            content=f\"Task: {task}\\nResponse: {response}\",\n            metadata={\n                'agent': self.name,\n                'timestamp': datetime.now().isoformat(),\n                'type': 'task_execution'\n            }\n        )\n        \n        # Step 6: Update conversation history\n        self.conversation_history.append({'role': 'user', 'content': task})\n        self.conversation_history.append({'role': 'assistant', 'content': response})\n        \n        # Step 7: Return structured response\n        return AgentResponse(\n            content=response,\n            confidence=0.8,\n            reasoning=\"Task executed with context and memory retrieval\",\n            metadata={'memories_used': len(relevant_memories)}\n        )\n\n# === Confirmation output ===\nprint(\"‚úÖ WorkerAgent loaded successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T06:23:32.863797Z","iopub.execute_input":"2025-11-27T06:23:32.865260Z","iopub.status.idle":"2025-11-27T06:23:32.874943Z","shell.execute_reply.started":"2025-11-27T06:23:32.865208Z","shell.execute_reply":"2025-11-27T06:23:32.873665Z"}},"outputs":[{"name":"stdout","text":"‚úÖ WorkerAgent loaded successfully.\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# Evaluator Agent (Quality Control)\n\nclass EvaluatorAgent(BaseAgent):\n    \"\"\"Agent that evaluates responses from other agents\"\"\"\n    \n    def __init__(self):\n        super().__init__(\n            name=\"Evaluator\",\n            role=\"Quality Assessor\",\n            system_prompt=\"\"\"You are an expert evaluator. Assess responses on:\n            1. Accuracy (0-1): Is the information correct?\n            2. Completeness (0-1): Does it fully answer the question?\n            3. Clarity (0-1): Is it easy to understand?\n            4. Relevance (0-1): Does it address the task?\n            \n            Provide scores and specific, actionable feedback for improvement.\n            Format your response as JSON with scores and feedback.\"\"\"\n        )\n    \n    def process(self, task: str, response: str) -> Dict[str, Any]:\n        \"\"\"Evaluate a response and provide detailed feedback\"\"\"\n        \n        # Construct evaluation prompt\n        evaluation_prompt = f\"\"\"\n        {self.system_prompt}\n        \n        Task: {task}\n        \n        Response to Evaluate: {response}\n        \n        Evaluate this response and provide:\n        1. Scores for accuracy, completeness, clarity, relevance (each 0-1)\n        2. Overall score (average of the four)\n        3. Specific feedback explaining what works and what doesn't\n        4. Concrete suggestions for improvement\n        \n        Return as JSON format:\n        {{\n            \"accuracy\": 0.0-1.0,\n            \"completeness\": 0.0-1.0,\n            \"clarity\": 0.0-1.0,\n            \"relevance\": 0.0-1.0,\n            \"overall_score\": 0.0-1.0,\n            \"feedback\": \"detailed feedback here\",\n            \"suggestions\": [\"suggestion 1\", \"suggestion 2\"]\n        }}\n        \"\"\"\n        \n        # Get evaluation from LLM\n        eval_response = self.call_llm(evaluation_prompt)\n        \n        # Parse evaluation (with fallback for safety)\n        try:\n            # Try to extract JSON from response\n            import re\n            json_match = re.search(r'\\{.*\\}', eval_response, re.DOTALL)\n            if json_match:\n                evaluation = json.loads(json_match.group())\n                # Ensure all required fields exist\n                if 'overall_score' not in evaluation:\n                    scores = [\n                        evaluation.get('accuracy', 0.7),\n                        evaluation.get('completeness', 0.7),\n                        evaluation.get('clarity', 0.7),\n                        evaluation.get('relevance', 0.7)\n                    ]\n                    evaluation['overall_score'] = sum(scores) / len(scores)\n            else:\n                # Fallback: create reasonable default evaluation\n                evaluation = {\n                    'accuracy': 0.75,\n                    'completeness': 0.70,\n                    'clarity': 0.80,\n                    'relevance': 0.85,\n                    'overall_score': 0.775,\n                    'feedback': eval_response,\n                    'suggestions': ['Consider adding more specific examples']\n                }\n        except Exception as e:\n            print(f\"‚ö†Ô∏è Evaluation parsing error: {e}\")\n            # Safe fallback\n            evaluation = {\n                'accuracy': 0.7,\n                'completeness': 0.7,\n                'clarity': 0.7,\n                'relevance': 0.7,\n                'overall_score': 0.7,\n                'feedback': eval_response,\n                'suggestions': ['Unable to parse specific suggestions']\n            }\n        \n        # Store evaluation in memory\n        self.memory.add_memory(\n            content=f\"Evaluated task: {task}\\nScore: {evaluation.get('overall_score', 0)}\",\n            metadata={\n                'agent': self.name,\n                'timestamp': datetime.now().isoformat(),\n                'type': 'evaluation',\n                'score': evaluation.get('overall_score', 0)\n            }\n        )\n        \n        return evaluation\n\n        \n# === Confirmation output ===\nprint(\"‚úÖ EvaluatorAgent loaded successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T06:23:36.442624Z","iopub.execute_input":"2025-11-27T06:23:36.442976Z","iopub.status.idle":"2025-11-27T06:23:36.455736Z","shell.execute_reply.started":"2025-11-27T06:23:36.442952Z","shell.execute_reply":"2025-11-27T06:23:36.454738Z"}},"outputs":[{"name":"stdout","text":"‚úÖ EvaluatorAgent loaded successfully.\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# Prompt Optimizer Agent (Makes things better)\n# -----------------------------------------------------\nclass PromptOptimizerAgent(BaseAgent):\n    \"\"\"Agent that optimizes prompts based on evaluation feedback\"\"\"\n    \n    def __init__(self):\n        super().__init__(\n            name=\"PromptOptimizer\",\n            role=\"Prompt Refiner\",\n            system_prompt=\"\"\"You are a prompt engineering expert.\n            Given a task, response, and evaluation, create an improved prompt\n            that addresses the identified weaknesses while maintaining clarity.\n            \n            Your optimized prompts should:\n            - Be more specific and detailed\n            - Include helpful constraints or examples\n            - Address evaluation feedback directly\n            - Guide toward higher quality responses\"\"\"\n        )\n    \n    def optimize_prompt(\n        self,\n        original_task: str,\n        response: str,\n        evaluation: Dict[str, Any]\n    ) -> str:\n        \"\"\"Generate an optimized version of the prompt\"\"\"\n        \n        # Build optimization prompt\n        optimization_prompt = f\"\"\"\n        {self.system_prompt}\n        \n        Original Task: {original_task}\n        \n        Previous Response: {response[:500]}...\n        \n        Evaluation Results:\n        Overall Score: {evaluation.get('overall_score', 0):.2f}\n        \n        Specific Scores:\n        - Accuracy: {evaluation.get('accuracy', 0):.2f}\n        - Completeness: {evaluation.get('completeness', 0):.2f}\n        - Clarity: {evaluation.get('clarity', 0):.2f}\n        - Relevance: {evaluation.get('relevance', 0):.2f}\n        \n        Feedback: {evaluation.get('feedback', 'No feedback')}\n        \n        Suggestions: {evaluation.get('suggestions', [])}\n        \n        Create an improved version of the original task prompt that:\n        1. Addresses ALL weaknesses identified in the evaluation\n        2. Provides clearer, more specific instructions\n        3. Includes helpful constraints, examples, or format requirements\n        4. Maintains the core intent of the original task\n        5. Guides toward a response that would score above 0.8\n        \n        Return ONLY the optimized prompt text, nothing else.\n        \"\"\"\n        \n        # Get optimized prompt from LLM\n        optimized = self.call_llm(optimization_prompt)\n        \n        # Store optimization in memory\n        self.memory.add_memory(\n            content=f\"Optimized prompt for: {original_task[:100]}...\\nNew prompt: {optimized[:100]}...\",\n            metadata={\n                'agent': self.name,\n                'timestamp': datetime.now().isoformat(),\n                'type': 'prompt_optimization',\n                'original_score': evaluation.get('overall_score', 0)\n            }\n        )\n        \n        return optimized.strip()\n\n\n# === Confirmation output ===\nprint(\"‚úÖ PromptOptimizerAgent loaded successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T06:23:40.682092Z","iopub.execute_input":"2025-11-27T06:23:40.682444Z","iopub.status.idle":"2025-11-27T06:23:40.692575Z","shell.execute_reply.started":"2025-11-27T06:23:40.682419Z","shell.execute_reply":"2025-11-27T06:23:40.691026Z"}},"outputs":[{"name":"stdout","text":"‚úÖ PromptOptimizerAgent loaded successfully.\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"# Multi-Agent Orchestrator (The Conductor)\n\nclass AgentOrchestrator:\n    \"\"\"Coordinates multiple agents for self-evaluation and iterative improvement\"\"\"\n    \n    def __init__(self):\n        # Initialize all agents\n        self.worker = WorkerAgent()\n        self.evaluator = EvaluatorAgent()\n        self.optimizer = PromptOptimizerAgent()\n        self.iteration_history = []\n    \n    def run_task_with_refinement(\n        self, \n        task: str, \n        max_iterations: int = Config.MAX_ITERATIONS\n    ) -> Dict[str, Any]:\n        \"\"\"Execute task with iterative refinement until quality threshold is met\"\"\"\n        \n        current_task = task\n        results = {\n            'original_task': task,\n            'iterations': [],\n            'final_response': None,\n            'final_score': 0\n        }\n        \n        # Iterative refinement loop\n        for iteration in range(max_iterations):\n            print(f\"\\n{'='*60}\")\n            print(f\"ITERATION {iteration + 1}/{max_iterations}\")\n            print(f\"{'='*60}\\n\")\n            \n            # STEP 1: Worker executes the task\n            print(f\"[{self.worker.name}] Executing task...\")\n            worker_response = self.worker.process(current_task)\n            response_preview = worker_response.content[:200].replace('\\n', ' ')\n            print(f\"Response preview: {response_preview}...\")\n            \n            # STEP 2: Evaluator assesses the response quality\n            print(f\"\\n[{self.evaluator.name}] Evaluating response...\")\n            evaluation = self.evaluator.process(current_task, worker_response.content)\n            print(f\"Overall Score: {evaluation.get('overall_score', 0):.2f}\")\n            print(f\"  - Accuracy: {evaluation.get('accuracy', 0):.2f}\")\n            print(f\"  - Completeness: {evaluation.get('completeness', 0):.2f}\")\n            print(f\"  - Clarity: {evaluation.get('clarity', 0):.2f}\")\n            print(f\"  - Relevance: {evaluation.get('relevance', 0):.2f}\")\n            \n            # Store iteration data\n            iteration_data = {\n                'iteration': iteration + 1,\n                'task': current_task,\n                'response': worker_response.content,\n                'evaluation': evaluation\n            }\n            results['iterations'].append(iteration_data)\n            \n            # STEP 3: Check if quality threshold is met\n            current_score = evaluation.get('overall_score', 0)\n            if current_score >= Config.EVALUATION_THRESHOLD:\n                print(f\"\\n‚úÖ Quality threshold met! (Score: {current_score:.2f} >= {Config.EVALUATION_THRESHOLD})\")\n                results['final_response'] = worker_response.content\n                results['final_score'] = current_score\n                break\n            \n            # STEP 4: Optimizer refines the prompt for next iteration\n            if iteration < max_iterations - 1:\n                print(f\"\\n[{self.optimizer.name}] Optimizing prompt for next iteration...\")\n                current_task = self.optimizer.optimize_prompt(\n                    current_task,\n                    worker_response.content,\n                    evaluation\n                )\n                task_preview = current_task[:200].replace('\\n', ' ')\n                print(f\"Optimized task preview: {task_preview}...\")\n            else:\n                # Max iterations reached\n                print(f\"\\n‚ö†Ô∏è Max iterations reached. Using best response so far.\")\n                results['final_response'] = worker_response.content\n                results['final_score'] = current_score\n        \n        return results\n    \n    def get_memory_summary(self) -> Dict[str, List[Dict]]:\n        \"\"\"Get summary of all agent memories\"\"\"\n        return {\n            'worker_memories': self.worker.memory.get_all_memories(),\n            'evaluator_memories': self.evaluator.memory.get_all_memories(),\n            'optimizer_memories': self.optimizer.memory.get_all_memories()\n        }\n\n\n# === Confirmation output ===\nprint(\"‚úÖ AgentOrchestrator loaded successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T06:23:48.847718Z","iopub.execute_input":"2025-11-27T06:23:48.848111Z","iopub.status.idle":"2025-11-27T06:23:48.862287Z","shell.execute_reply.started":"2025-11-27T06:23:48.848081Z","shell.execute_reply":"2025-11-27T06:23:48.861080Z"}},"outputs":[{"name":"stdout","text":"‚úÖ AgentOrchestrator loaded successfully.\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"def visualize_improvement(results):\n    \"\"\"Line graph showing overall score improvement across iterations\"\"\"\n    try:\n        import matplotlib.pyplot as plt\n        \n        iterations = [i['iteration'] for i in results['iterations']]\n        scores = [i['evaluation'].get('overall_score', 0) for i in results['iterations']]\n        \n        plt.figure(figsize=(12, 7))\n        plt.plot(iterations, scores, marker='o', linewidth=3, markersize=10,\n                 label='Overall Score')\n\n        plt.axhline(y=Config.EVALUATION_THRESHOLD, linestyle='--', linewidth=2,\n                    label=f'Target Threshold ({Config.EVALUATION_THRESHOLD})')\n        \n        plt.xlabel('Iteration', fontsize=14, fontweight='bold')\n        plt.ylabel('Evaluation Score', fontsize=14, fontweight='bold')\n        plt.title('LLM Self-Evaluation: Score Improvement Over Iterations',\n                  fontsize=16, fontweight='bold')\n        plt.grid(True, alpha=0.3)\n        plt.legend()\n        plt.xticks(iterations)\n        plt.ylim(0, 1.05)\n\n        for i, score in zip(iterations, scores):\n            plt.annotate(f'{score:.2f}', xy=(i, score), xytext=(0, 10),\n                         textcoords='offset points', ha='center')\n\n        plt.tight_layout()\n        plt.show()\n        \n    except Exception as e:\n        print(f\"‚ö†Ô∏è Line graph error: {e}\")\n\n\ndef visualize_bar_graph(results):\n    \"\"\"Bar graph comparing metrics across all iterations\"\"\"\n    try:\n        import matplotlib.pyplot as plt\n        import numpy as np\n        \n        iterations = [f\"Iter {i['iteration']}\" for i in results['iterations']]\n        accuracy = [i['evaluation'].get('accuracy', 0) for i in results['iterations']]\n        completeness = [i['evaluation'].get('completeness', 0) for i in results['iterations']]\n        clarity = [i['evaluation'].get('clarity', 0) for i in results['iterations']]\n        relevance = [i['evaluation'].get('relevance', 0) for i in results['iterations']]\n        \n        x = np.arange(len(iterations))\n        width = 0.2\n        \n        fig, ax = plt.subplots(figsize=(14, 7))\n        \n        ax.bar(x - 1.5*width, accuracy, width, label='Accuracy')\n        ax.bar(x - 0.5*width, completeness, width, label='Completeness')\n        ax.bar(x + 0.5*width, clarity, width, label='Clarity')\n        ax.bar(x + 1.5*width, relevance, width, label='Relevance')\n        \n        ax.set_xlabel('Iterations', fontsize=14, fontweight='bold')\n        ax.set_ylabel('Scores', fontsize=14, fontweight='bold')\n        ax.set_title('LLM Evaluation Metrics Across Iterations',\n                     fontsize=16, fontweight='bold')\n        ax.set_xticks(x)\n        ax.set_xticklabels(iterations)\n        ax.legend()\n        ax.grid(True, alpha=0.3, axis='y')\n        ax.set_ylim(0, 1.05)\n\n        plt.tight_layout()\n        plt.show()\n        \n    except Exception as e:\n        print(f\"‚ö†Ô∏è Bar graph error: {e}\")\n\n\ndef visualize_radar_chart(results):\n    \"\"\"Radar chart for final iteration metric distribution\"\"\"\n    try:\n        import matplotlib.pyplot as plt\n        import numpy as np\n        \n        final_eval = results['iterations'][-1]['evaluation']\n        \n        categories = ['Accuracy', 'Completeness', 'Clarity', 'Relevance']\n        values = [\n            final_eval.get('accuracy', 0),\n            final_eval.get('completeness', 0),\n            final_eval.get('clarity', 0),\n            final_eval.get('relevance', 0)\n        ]\n        \n        N = len(categories)\n        angles = [n / float(N) * 2 * np.pi for n in range(N)]\n        values += values[:1]\n        angles += angles[:1]\n\n        fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n        \n        ax.plot(angles, values, 'o-', linewidth=2, label='Final Iteration')\n        ax.fill(angles, values, alpha=0.25)\n\n        ax.set_ylim(0, 1)\n        ax.set_xticks(angles[:-1])\n        ax.set_xticklabels(categories)\n        ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n        \n        plt.title('Final Evaluation Metric Profile (Radar Chart)',\n                  fontsize=16, fontweight='bold')\n        plt.legend()\n        \n        plt.tight_layout()\n        plt.show()\n        \n    except Exception as e:\n        print(f\"‚ö†Ô∏è Radar chart error: {e}\")\n\n\ndef visualize_confusion_matrix(results):\n    \"\"\"Confusion-style matrix showing predicted vs actual LLM evaluation scores\"\"\"\n    try:\n        import matplotlib.pyplot as plt\n        import numpy as np\n\n        final_eval = results['iterations'][-1]['evaluation']\n\n        metrics = ['Accuracy', 'Completeness', 'Clarity', 'Relevance']\n        \n        predicted = [\n            final_eval.get('pred_accuracy', final_eval.get('accuracy', 0)),\n            final_eval.get('pred_completeness', final_eval.get('completeness', 0)),\n            final_eval.get('pred_clarity', final_eval.get('clarity', 0)),\n            final_eval.get('pred_relevance', final_eval.get('relevance', 0))\n        ]\n        \n        actual = [\n            final_eval.get('actual_accuracy', final_eval.get('accuracy', 0)),\n            final_eval.get('actual_completeness', final_eval.get('completeness', 0)),\n            final_eval.get('actual_clarity', final_eval.get('clarity', 0)),\n            final_eval.get('actual_relevance', final_eval.get('relevance', 0))\n        ]\n\n        matrix = np.array([predicted, actual])\n\n        fig, ax = plt.subplots(figsize=(10, 5))\n        im = ax.imshow(matrix, cmap='Blues', vmin=0, vmax=1)\n\n        ax.set_xticks(np.arange(len(metrics)))\n        ax.set_yticks([0, 1])\n        ax.set_xticklabels(metrics)\n        ax.set_yticklabels(['Predicted', 'Actual'])\n\n        for i in range(2):\n            for j in range(len(metrics)):\n                ax.text(j, i, f'{matrix[i, j]:.2f}',\n                        ha='center', va='center')\n\n        plt.title(\"Predicted vs Actual Evaluation Scores (Confusion-Style Matrix)\",\n                  fontsize=16, fontweight='bold')\n        plt.colorbar(im)\n\n        plt.tight_layout()\n        plt.show()\n\n    except Exception as e:\n        print(f\"‚ö†Ô∏è Confusion matrix error: {e}\")\n\n\ndef visualize_all(results):\n    \"\"\"Run all 4 visualizations\"\"\"\n    print(\"=\"*70)\n    print(\"üìä GENERATING LLM SELF-EVALUATION VISUALIZATIONS\")\n    print(\"=\"*70 + \"\\n\")\n    \n    print(\"1Ô∏è‚É£ Line Graph - Score Progression\")\n    visualize_improvement(results)\n    \n    print(\"\\n2Ô∏è‚É£ Bar Graph - Metric Comparison\")\n    visualize_bar_graph(results)\n    \n    print(\"\\n3Ô∏è‚É£ Radar Chart - Final Evaluation Shape\")\n    visualize_radar_chart(results)\n\n    print(\"\\n4Ô∏è‚É£ Confusion-Style Matrix - Predicted vs Actual\")\n    visualize_confusion_matrix(results)\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"‚úÖ ALL VISUALIZATIONS COMPLETE!\")\n    print(\"=\"*70)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T06:23:54.672642Z","iopub.execute_input":"2025-11-27T06:23:54.673014Z","iopub.status.idle":"2025-11-27T06:23:54.699688Z","shell.execute_reply.started":"2025-11-27T06:23:54.672988Z","shell.execute_reply":"2025-11-27T06:23:54.698704Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"def create_interactive_interface():\n    \"\"\"Create an interactive chat-like interface for the agent system\"\"\"\n    try:\n        from IPython.display import display, HTML, clear_output\n        import ipywidgets as widgets\n        \n        print(\"‚úÖ Interactive interface loaded!\")\n        \n        # Create orchestrator\n        orchestrator = AgentOrchestrator()\n        \n        # Create UI components\n        output_area = widgets.Output()\n        \n        # Input text area\n        prompt_input = widgets.Textarea(\n            value='Explain quantum computing to a beginner with a real-world example',\n            placeholder='Enter your task here...',\n            description='Your Task:',\n            layout=widgets.Layout(width='100%', height='100px'),\n            style={'description_width': '100px'}\n        )\n        \n        # Settings\n        iterations_slider = widgets.IntSlider(\n            value=3,\n            min=1,\n            max=5,\n            step=1,\n            description='Max Iterations:',\n            style={'description_width': '120px'}\n        )\n        \n        threshold_slider = widgets.FloatSlider(\n            value=0.7,\n            min=0.5,\n            max=1.0,\n            step=0.05,\n            description='Quality Target:',\n            style={'description_width': '120px'}\n        )\n        \n        # Buttons\n        run_button = widgets.Button(\n            description='üöÄ Run Agent System',\n            button_style='success',\n            layout=widgets.Layout(width='200px', height='40px')\n        )\n        \n        clear_button = widgets.Button(\n            description='üóëÔ∏è Clear Output',\n            button_style='warning',\n            layout=widgets.Layout(width='200px', height='40px')\n        )\n        \n        visualize_button = widgets.Button(\n            description='üìà Line Graph',\n            button_style='info',\n            layout=widgets.Layout(width='200px', height='40px')\n        )\n        \n        bar_graph_button = widgets.Button(\n            description='üìä Bar Graph',\n            button_style='info',\n            layout=widgets.Layout(width='200px', height='40px')\n        )\n        \n        radar_button = widgets.Button(\n            description='üéØ Radar Chart',\n            button_style='info',\n            layout=widgets.Layout(width='200px', height='40px')\n        )\n        \n        confusion_button = widgets.Button(\n            description='üîµ Confusion Matrix',\n            button_style='info',\n            layout=widgets.Layout(width='200px', height='40px')\n        )\n        \n        all_viz_button = widgets.Button(\n            description='üé® All Visualizations',\n            button_style='primary',\n            layout=widgets.Layout(width='200px', height='40px')\n        )\n        \n        # Store global results\n        global last_results\n        last_results = None\n        \n        # Handler for running agent\n        def on_run_click(b):\n            global last_results\n            with output_area:\n                clear_output()\n                \n                display(HTML(\"\"\"\n                    <div style='background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); \n                                padding: 20px; border-radius: 10px; color: white; margin-bottom: 20px;'>\n                        <h2 style='margin: 0;'>ü§ñ AI Agent System Processing...</h2>\n                        <p>Watch your prompt being refined...</p>\n                    </div>\n                \"\"\"))\n                \n                task = prompt_input.value\n                max_iter = iterations_slider.value\n                Config.EVALUATION_THRESHOLD = threshold_slider.value\n                \n                print(f\"üìù Task: {task}\\n\")\n                print(f\"‚öôÔ∏è Iterations={max_iter}, Target Score={threshold_slider.value}\\n\")\n                \n                results = orchestrator.run_task_with_refinement(task, max_iterations=max_iter)\n                last_results = results\n                \n                display(HTML(f\"\"\"\n                    <div style='background: #e8f4ff; padding: 20px; border-radius: 10px; \n                                border-left: 6px solid #4285F4;'>\n                        <h3>üéØ Final Results</h3>\n                        <p><b>Total Iterations:</b> {len(results['iterations'])}</p>\n                        <p><b>Final Score:</b> <span style='color:#0f9d58;font-size:22px;'>{results['final_score']:.2f}</span></p>\n                    </div>\n                \"\"\"))\n                \n                display(HTML(\"<h4>üìù Final Optimized Response:</h4>\"))\n                print(results['final_response'])\n        \n        # ‚ùó FIXED: attach event handler to run button\n        run_button.on_click(on_run_click)\n        \n        # Visualization handler\n        def safe_viz(func, name):\n            global last_results\n            with output_area:\n                if last_results:\n                    print(f\"\\nüìä Generating {name}...\\n\")\n                    func(last_results)\n                else:\n                    print(\"‚ö†Ô∏è No results yet. Run the agent first!\")\n        \n        visualize_button.on_click(lambda b: safe_viz(visualize_improvement, \"Line Graph\"))\n        bar_graph_button.on_click(lambda b: safe_viz(visualize_bar_graph, \"Bar Graph\"))\n        radar_button.on_click(lambda b: safe_viz(visualize_radar_chart, \"Radar Chart\"))\n        confusion_button.on_click(lambda b: safe_viz(visualize_confusion_matrix, \"Confusion Matrix\"))\n        all_viz_button.on_click(lambda b: safe_viz(visualize_all, \"All Visualizations\"))\n        \n        clear_button.on_click(lambda b: (output_area.clear_output(), print(\"Output cleared!\")))\n        \n        # Layout\n        settings_box = widgets.VBox([iterations_slider, threshold_slider])\n        \n        buttons_box = widgets.HBox([run_button, clear_button, visualize_button])\n        \n        viz_buttons_box = widgets.HBox([\n            bar_graph_button,\n            radar_button,\n            confusion_button,\n            all_viz_button\n        ], layout=widgets.Layout(gap='10px', margin='10px 0'))\n        \n        interface = widgets.VBox([\n            widgets.HTML(\"\"\"\n                <div style='background: linear-gradient(135deg,#667eea,#764ba2); \n                            padding:25px; border-radius:15px; color:white;text-align:center;'>\n                    <h1>ü§ñ LLM Self-Evaluation Agent System</h1>\n                    <p>Google Gemini API + Multi-Agent Pipeline</p>\n                </div>\n            \"\"\"),\n            prompt_input,\n            settings_box,\n            buttons_box,\n            viz_buttons_box,\n            output_area\n        ])\n        \n        display(interface)\n    \n    except Exception as e:\n        print(f\"‚ùå Error: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T06:24:00.842030Z","iopub.execute_input":"2025-11-27T06:24:00.842326Z","iopub.status.idle":"2025-11-27T06:24:00.860209Z","shell.execute_reply.started":"2025-11-27T06:24:00.842304Z","shell.execute_reply":"2025-11-27T06:24:00.858970Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"# Launch Interactive Interface\n# Run this cell to launch the interactive UI!\n\nprint(\"üöÄ Launching Interactive Interface...\")\nprint(\"=\"*70 + \"\\n\")\ncreate_interactive_interface()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T06:24:06.316341Z","iopub.execute_input":"2025-11-27T06:24:06.316688Z","iopub.status.idle":"2025-11-27T06:24:19.119760Z","shell.execute_reply.started":"2025-11-27T06:24:06.316664Z","shell.execute_reply":"2025-11-27T06:24:19.118251Z"}},"outputs":[{"name":"stdout","text":"üöÄ Launching Interactive Interface...\n======================================================================\n\n‚úÖ Interactive interface loaded!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a33731929a2449808f9eda4bef297b23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5e0fc2de5d246eead101623162596b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"050eb5a1a24f4a91ae9f6ce70af4dcec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72f0ad022ab149648acac151252508a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"146852dde58547908ed86f1115ae431d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e37c1a08a86487289c1a0895c5c1de2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68e551a92046463a9f0946e21f4ef1cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69936d6d422543929015eed6484151ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c4956e094ec4f80816fed33f73fa3c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5adfd689d544a3e9d66952ae8a6fe2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afc86d21145b4a66a95e9cd7fee57870"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value=\"\\n                <div style='background: linear-gradient(135deg,#667eea,#764ba2); ‚Ä¶","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffe671b1c6d14e549145db07f3e31978"}},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"# Example Usage & Demo\n\ndef demo_agent_system():\n    \"\"\"Demonstrate the complete agent system\"\"\"\n    \n    print(\"=\"*70)\n    print(\"ü§ñ LLM SELF-EVALUATION & PROMPT-FINETUNING SYSTEM\")\n    print(\"   Powered by Google Gemini API\")\n    print(\"=\"*70)\n    print(\"\\nThis system uses 3 AI agents working together:\")\n    print(\"  1. Worker: Executes tasks\")\n    print(\"  2. Evaluator: Assesses quality\")\n    print(\"  3. Optimizer: Improves prompts\")\n    print(\"\\n\" + \"=\"*70 + \"\\n\")\n    \n    # Initialize orchestrator\n    orchestrator = AgentOrchestrator()\n    \n    # üéØ YOUR TASK GOES HERE - Change this to anything you want or anything of your choice!\n    # Examples:\n    # - \"Explain quantum computing to a 10-year-old\"\n    # - \"Write a Python function to find prime numbers\"\n    # - \"Create a marketing strategy for a local bakery\"\n    # - \"Explain the water cycle with a creative analogy\"\n    \n    task = \"\"\"Explain quantum computing to a 10-year-old\"\"\"\n    \n    print(f\"üìù Original Task:\\n{task}\\n\")\n    print(\"=\"*70)\n    \n    # Run task with iterative refinement\n    results = orchestrator.run_task_with_refinement(task, max_iterations=3)\n    \n    # Display final results\n    print(\"\\n\" + \"=\"*70)\n    print(\"üéØ FINAL RESULTS\")\n    print(\"=\"*70)\n    print(f\"\\nüìä Total Iterations: {len(results['iterations'])}\")\n    print(f\"‚≠ê Final Score: {results['final_score']:.2f}\")\n    print(f\"\\nüìù Final Response:\\n\")\n    print(\"-\" * 70)\n    print(results['final_response'])\n    print(\"-\" * 70)\n    \n    # Show score progression\n    print(\"\\nüìà Score Progression:\")\n    for i, iter_data in enumerate(results['iterations']):\n        score = iter_data['evaluation'].get('overall_score', 0)\n        bar_length = int(score * 30)\n        bar = \"‚ñà\" * bar_length + \"‚ñë\" * (30 - bar_length)\n        print(f\"  Iteration {i+1}: {bar} {score:.2f}\")\n    \n    print(\"\\n\" + \"=\"*70)\n    \n    return results\n\n# === Confirmation output ===\nprint(\"\\n‚úÖ demo_agent_system() executed successfully!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Execute this cell to see the system in action! demo run\n\nif __name__ == \"__main__\":\n    results = demo_agent_system()\n\n# Optional: Uncomment below to see detailed iteration data\n# print(\"\\n\\nDetailed iteration 1 data:\")\n# print(json.dumps(results['iterations'][0], indent=2))","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}